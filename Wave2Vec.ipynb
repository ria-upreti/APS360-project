{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "collapsed_sections": [
        "Moh5PVtEbiCl"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Constants"
      ],
      "metadata": {
        "id": "KibS8wPkZyl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading"
      ],
      "metadata": {
        "id": "lseH8HE5Us81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VGyMibX7aLvk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e843c452-c833-41bf-9d98-7b61b5dafe9e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LANGUAGES = [\"en\", \"de\", \"nl\", \"sv-SE\", \"da\"]\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 1\n",
        "TRAIN_BATCH = 1200\n",
        "VAL_BATCH = 300\n",
        "SAMPLE_RATE = 16000\n",
        "\n",
        "# Constants related to the MFCC processing\n",
        "# the number of samples per fft\n",
        "N_FFT = 2048\n",
        "# the amount of transform to shift\n",
        "HOP_LENGTH = 512\n",
        "# The number of coefficient we extract\n",
        "N_MFCC = 13"
      ],
      "metadata": {
        "id": "mwcVX2wFWyy8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Processing"
      ],
      "metadata": {
        "id": "fVqohc6HSxV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from glob import glob # list out files in directory -> reading wave files\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as ipd # play files\n",
        "\n",
        "from itertools import cycle # colours and gimiks"
      ],
      "metadata": {
        "id": "XpIwtX0RS15s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "T2rbl8V_TChV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "9-1u9L9mTB-a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Dataset and Mount Drive"
      ],
      "metadata": {
        "id": "aiRpXsylZoC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YJ5UjvQhyABD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "e589d674-6d52-4a05-921e-7d8a7ee6022f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# linking hugging face account\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pLqETfMgs6vk",
        "outputId": "22c2649e-f320-4927-da7c-b9b34f704935"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `Sneed` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `Sneed`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Don't load the CNN"
      ],
      "metadata": {
        "id": "WyN_HCNOK7sE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def load_data(languages, train_batch, val_batch, randomOn=False):\n",
        "  train_data = []\n",
        "  val_data = []\n",
        "\n",
        "  train_audio = []\n",
        "  train_labels = []\n",
        "  train_sr = []\n",
        "  val_audio = []\n",
        "  val_labels = []\n",
        "  val_sr = []\n",
        "\n",
        "  one_hot = F.one_hot(torch.tensor([0, 1, 2, 3, 4]), num_classes=len(languages))\n",
        "\n",
        "  for i in range(len(one_hot)):\n",
        "    one_hot[i] = one_hot[i].to(dtype=torch.float)\n",
        "\n",
        "  for i in range(len(languages)):\n",
        "    # Load common voice 17 dataset training set with streaming, and enabling custom code (necessary to load dataset correctly)\n",
        "    if randomOn:\n",
        "      train_set = load_dataset(\"mozilla-foundation/common_voice_17_0\", languages[i], split=\"train\", streaming=True, trust_remote_code=True)\n",
        "      train_data.append(train_set.shuffle(buffer_size=train_batch, seed=random.randrange(1,100)))\n",
        "      val_set = load_dataset(\"mozilla-foundation/common_voice_17_0\", languages[i], split=\"validation\", streaming=True, trust_remote_code=True)\n",
        "      val_data.append(val_set.shuffle(buffer_size=val_batch, seed=random.randrange(1,100)))\n",
        "    else:\n",
        "      train_data.append(load_dataset(\"mozilla-foundation/common_voice_17_0\", languages[i], split=\"train\", streaming=True, cache_dir='/content/my_cache', trust_remote_code=True))\n",
        "      val_data.append(load_dataset(\"mozilla-foundation/common_voice_17_0\", languages[i], split=\"validation\", streaming=True, cache_dir='/content/my_cache', trust_remote_code=True))\n",
        "\n",
        "    it = iter(train_data[i])\n",
        "    it2 = iter(val_data[i])\n",
        "\n",
        "    for j in range(train_batch):\n",
        "      train_item = next(it)\n",
        "\n",
        "      if train_item:\n",
        "        train_audio.append(train_item['audio']['array'])\n",
        "        train_sr.append(train_item['audio']['sampling_rate'])\n",
        "        train_labels.append(one_hot[i])\n",
        "\n",
        "    for j in range(val_batch):\n",
        "      val_item = next(it2)\n",
        "\n",
        "      if val_item:\n",
        "        val_audio.append(val_item['audio']['array'])\n",
        "        val_sr.append(val_item['audio']['sampling_rate'])\n",
        "        val_labels.append(one_hot[i])\n",
        "\n",
        "\n",
        "    print(f\"Loaded {languages[i]}\")\n",
        "\n",
        "  return train_audio, train_labels, train_sr, val_audio, val_labels, val_sr"
      ],
      "metadata": {
        "id": "kcxGewtCt1YQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_audio, train_labels, train_sr, val_audio, val_labels, val_sr = load_data(LANGUAGES, TRAIN_BATCH, VAL_BATCH, randomOn=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-xEGTzsyPY8",
        "outputId": "8f2d1cf8-9534-4a4c-9698-1d2480293f33"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading metadata...: 1101170it [00:27, 39650.83it/s]\n",
            "Reading metadata...: 16393it [00:00, 34757.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded en\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading metadata...: 589100it [00:13, 42500.86it/s]\n",
            "Reading metadata...: 16183it [00:00, 44529.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded de\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading metadata...: 34898it [00:00, 41800.92it/s]\n",
            "Reading metadata...: 11252it [00:00, 34906.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded nl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading metadata...: 7744it [00:01, 4596.14it/s]\n",
            "Reading metadata...: 5210it [00:00, 28458.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded sv-SE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading metadata...: 3484it [00:00, 12778.68it/s]\n",
            "Reading metadata...: 2105it [00:00, 19050.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded da\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "V-rmiEypZrQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "YotWu-lGbbUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_batch(audio_data, sample_rates, batch_size, mfcc=True):\n",
        "  audio_processed = []\n",
        "  LENGTH =  5 * SAMPLE_RATE\n",
        "\n",
        "  for i in range(batch_size):\n",
        "    audio_resampled = librosa.resample(audio_data[i], orig_sr = sample_rates[i], target_sr = SAMPLE_RATE)\n",
        "\n",
        "    audio_trimmed, _ = librosa.effects.trim(audio_resampled, top_db=60)\n",
        "\n",
        "    if len(audio_trimmed) > LENGTH:\n",
        "      audio_trimmed = audio_trimmed[:LENGTH]\n",
        "\n",
        "    elif len(audio_trimmed) < LENGTH:\n",
        "      padding =  LENGTH - len(audio_trimmed)\n",
        "      audio_trimmed = np.pad(audio_trimmed, (0, padding), mode='constant')\n",
        "\n",
        "    if mfcc:\n",
        "      audio_trimmed = librosa.feature.mfcc(y = audio_trimmed, sr=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mfcc=N_MFCC)\n",
        "\n",
        "    audio_processed.append(audio_trimmed)\n",
        "\n",
        "    if i % TRAIN_BATCH == 0:\n",
        "      print(f\"Processed {i} audio files\")\n",
        "\n",
        "  return audio_processed"
      ],
      "metadata": {
        "id": "mxFlPmh3HiLb"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib import cm\n",
        "\n",
        "TEST_INDEX = 1\n",
        "\n",
        "print(len(train_audio))\n",
        "MFCC_train = process_batch(train_audio, train_sr, len(train_audio))\n",
        "\n",
        "D = librosa.amplitude_to_db(np.abs(MFCC_train[TEST_INDEX]), ref=np.max)\n",
        "img = librosa.display.specshow(D, y_axis='linear', x_axis='time', sr=SAMPLE_RATE)\n",
        "\n",
        "print(MFCC_train[TEST_INDEX].shape)"
      ],
      "metadata": {
        "id": "ydd64wXst82d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "outputId": "b30af1a9-a430-45d2-c38d-c553be4383a8"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6000\n",
            "Processed 0 audio files\n",
            "Processed 1200 audio files\n",
            "Processed 2400 audio files\n",
            "Processed 3600 audio files\n",
            "Processed 4800 audio files\n",
            "(13, 469)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MFCC_val = process_batch(val_audio, val_sr, len(val_audio))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de0Jtd2QlfSY",
        "outputId": "c8cea2db-7213-442f-9061-6f6dd1ab2526"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 0 audio files\n",
            "Processed 1200 audio files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Class Archive(Old, do not use)"
      ],
      "metadata": {
        "id": "Moh5PVtEbiCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "class AudioDataset_old(Dataset):\n",
        "    def __init__(self, audios, labels, sample_rates, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mfcc=N_MFCC, transform=None):\n",
        "        \"\"\"\n",
        "        audios: list of raw audio arrays\n",
        "        labels: list of labels\n",
        "        \"\"\"\n",
        "        self.audios = audios\n",
        "        self.labels = labels\n",
        "        self.sample_rates = sample_rates\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "        self.n_mfcc = n_mfcc\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audios)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio = self.audios[idx]\n",
        "        label = self.labels[idx]\n",
        "        sr = self.sample_rates[idx]\n",
        "\n",
        "        audio_length = SAMPLE_RATE*5\n",
        "\n",
        "        # Resample\n",
        "        audio_resampled = librosa.resample(audio, orig_sr = sr, target_sr = SAMPLE_RATE)\n",
        "\n",
        "        # Trimming decibels\n",
        "        audio_trimmed, _ = librosa.effects.trim(audio_resampled, top_db=80)\n",
        "\n",
        "        # if audio too long trim down length (5 secs)\n",
        "        if len(audio_trimmed) > audio_length:\n",
        "          audio_trimmed = audio_trimmed[:audio_length]\n",
        "\n",
        "        # if audio too short, add padding\n",
        "        elif len(audio_trimmed) < audio_length:\n",
        "          padding = audio_length - len(audio_trimmed)\n",
        "          audio_trimmed = np.pad(audio_trimmed, (0, padding), mode='constant')\n",
        "\n",
        "        # Calculate MFCCs for the trimmed audio\n",
        "        mfcc = librosa.feature.mfcc(y = np.abs(audio_trimmed), sr=SAMPLE_RATE, n_fft=self.n_fft, hop_length=self.hop_length, n_mfcc=self.n_mfcc)\n",
        "        mfcc = torch.tensor(mfcc, dtype=torch.float)\n",
        "\n",
        "        return mfcc, label"
      ],
      "metadata": {
        "id": "qgaDPrt1VTkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# audio_dataset = AudioDataset_old(train_audio, train_labels, train_sr)\n",
        "# train_loader = DataLoader(audio_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "ZMRya2RSQtDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Class"
      ],
      "metadata": {
        "id": "TA7bo20idJQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, processed_audio, labels, sample_rates, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mfcc=N_MFCC, transform=None):\n",
        "        \"\"\"\n",
        "        audios: list of raw audio arrays\n",
        "        labels: list of labels\n",
        "        \"\"\"\n",
        "        self.audios = processed_audio\n",
        "        self.labels = labels\n",
        "        self.sample_rates = sample_rates\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "        self.n_mfcc = n_mfcc\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audios)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio = self.audios[idx]\n",
        "        label = self.labels[idx]\n",
        "        audio = torch.tensor(audio, dtype=torch.float)\n",
        "\n",
        "        return audio, label"
      ],
      "metadata": {
        "id": "ySniNZ6udIU_"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_dataset = AudioDataset(MFCC_train, train_labels, train_sr)\n",
        "train_loader = DataLoader(audio_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "JxseEcH4dQ68"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_dataset = AudioDataset(MFCC_val, val_labels, val_sr)\n",
        "val_loader = DataLoader(audio_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "WMPpgh7fmBMf"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for data, label in train_loader:\n",
        "  if i == 3:\n",
        "    print(data.shape)\n",
        "    print(label)\n",
        "  i = i + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTYt-7kBdyop",
        "outputId": "3e829ba1-6078-4cda-95ae-29363bd68759"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 13, 469])\n",
            "tensor([[0, 0, 1, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Model"
      ],
      "metadata": {
        "id": "9y5V7V3rZvFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvNetwork(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=(3, 3), padding='same'),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding='same'),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding='same'),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, None))\n",
        "        )\n",
        "\n",
        "        self.gru_input_size = 128\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=self.gru_input_size,\n",
        "            hidden_size=256,\n",
        "            num_layers=2,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "            dropout=0.3\n",
        "        )\n",
        "\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(512, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Expected shape of x: [batch_size, time, n_mfcc]\n",
        "        We first unsqueeze to add the channel dimension: -> [batch_size, 1, n_mfcc, time]\n",
        "        Make sure n_mfcc is the 'frequency' dimension and time is the last dimension.\n",
        "        \"\"\"\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        x = self.conv_layers(x)\n",
        "\n",
        "        x = x.squeeze(2)\n",
        "\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        gru_out, _ = self.gru(x)\n",
        "\n",
        "        attention_scores = self.attention(gru_out)\n",
        "        attention_weights = F.softmax(attention_scores, dim=1)  # across time\n",
        "        context = torch.sum(attention_weights * gru_out, dim=1)\n",
        "\n",
        "        logits = self.classifier(context)\n",
        "        return logits, F.softmax(logits, dim=1)"
      ],
      "metadata": {
        "id": "vei5rbFQa7RJ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "avfuq0xQaoYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_result(model_code, train_err, train_loss, val_err, val_loss):\n",
        "  save_dir = \"/content/drive/My Drive/training_results\"\n",
        "  os.makedirs(save_dir, exist_ok=True)\n",
        "  model_path = os.path.join(save_dir, model_code)\n",
        "  np.savetxt(\"{}_train_err.csv\".format(model_path), train_err)\n",
        "  np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n",
        "  np.savetxt(\"{}_val_err.csv\".format(model_path), val_err)\n",
        "  np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)\n",
        "\n",
        "\n",
        "def save_weight(net, model_code):\n",
        "  \"\"\"model code will be a string code we assigned to each model\"\"\"\n",
        "  weight_dir = \"/content/drive/My Drive/model_weights\"\n",
        "  os.makedirs(weight_dir, exist_ok=True)\n",
        "\n",
        "  model_path = os.path.join(weight_dir, model_code)\n",
        "  torch.save(net.state_dict(), f\"{model_path}.pth\")\n",
        "\n",
        "\n",
        "def load_weight(net, model_code):\n",
        "  \"\"\"model code will be a string code we assigned to each model\"\"\"\n",
        "  weight_dir = \"/content/drive/My Drive/model_weights\"\n",
        "  model_path = os.path.join(weight_dir, model_code)\n",
        "\n",
        "  net.load_state_dict(torch.load(f\"{model_path}.pth\"))\n",
        "  net.eval()\n",
        "  return net"
      ],
      "metadata": {
        "id": "tWfyi7JtyDI9"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_curve(model_code):\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    save_dir = \"/content/drive/My Drive/training_results\"\n",
        "    path = os.path.join(save_dir, model_code)\n",
        "\n",
        "    train_err = np.loadtxt(\"{}_train_err.csv\".format(path))\n",
        "    val_err = np.loadtxt(\"{}_val_err.csv\".format(path))\n",
        "    train_loss = np.loadtxt(\"{}_train_loss.csv\".format(path))\n",
        "    val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\n",
        "\n",
        "    # plt.title(\"Train vs Validation Error\")\n",
        "    plt.title(\"Train Error\")\n",
        "    n = len(train_err) # number of epochs\n",
        "    plt.plot(range(1,n+1), train_err, label=\"Train\")\n",
        "    # plt.plot(range(1,n+1), val_err, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Error\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "    # plt.title(\"Train vs Validation Loss\")\n",
        "    plt.title(\"Train Loss\")\n",
        "    plt.plot(range(1,n+1), train_loss, label=\"Train\")\n",
        "    # plt.plot(range(1,n+1), val_loss, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "IKchax2xmhJC"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, model_name, data_loader, device, optimizer, loss_function, num_epochs, val_loader):\n",
        "  start_time = time.time()\n",
        "  train_err = np.zeros(num_epochs)\n",
        "  train_loss = np.zeros(num_epochs)\n",
        "  val_err = np.zeros(num_epochs)\n",
        "  val_loss = np.zeros(num_epochs)\n",
        "  total_train_err = 0\n",
        "  total_train_loss = 0\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    epoch_loss = 0.0\n",
        "    num_samples = 0\n",
        "    for data, label in data_loader:\n",
        "      data, label = data.to(DEVICE), label.to(DEVICE)\n",
        "      logits, predictions = model(data)\n",
        "      int_labels = label.argmax(dim=1)\n",
        "      loss = loss_function(logits, int_labels)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      pred_classes = torch.argmax(logits, dim=1)\n",
        "      true_classes = int_labels\n",
        "      correct += (pred_classes == true_classes).sum().item()\n",
        "      total += label.size(0)\n",
        "\n",
        "      epoch_loss += loss.item() * data.size(0)\n",
        "      num_samples += data.size(0)\n",
        "\n",
        "    train_loss[epoch] = epoch_loss / num_samples\n",
        "    train_err[epoch] = 100 - correct / total * 100\n",
        "    model.eval()\n",
        "    val_err[epoch], val_loss[epoch] = test_accuracy(model, val_loader, loss_function)\n",
        "    model.train()\n",
        "\n",
        "    save_weight(model, f\"{model_name}_epoch_{epoch + 1}\")\n",
        "    print(f\"Epoch #{epoch + 1} | Loss: {train_loss[epoch]} | Error: {train_err[epoch]}% | Loss: {val_loss[epoch]} | Error: {val_err[epoch]}%\")\n",
        "\n",
        "  save_result(model_name, train_err, train_loss, val_err, val_loss)\n",
        "\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(f\"Time elapsed: {elapsed_time:.4f} seconds\")"
      ],
      "metadata": {
        "id": "0gO0K21ta7sG"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r-QxiZbGg9sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_label(labels):\n",
        "    \"\"\"\n",
        "    Given a tensor containing 2 possible values, normalize this to 0/1\n",
        "\n",
        "    Args:\n",
        "        labels: a 1D tensor containing two possible scalar values\n",
        "    Returns:\n",
        "        A tensor normalize to 0/1 value\n",
        "    \"\"\"\n",
        "    max_val = torch.max(labels)\n",
        "    min_val = torch.min(labels)\n",
        "    norm_labels = (labels - min_val)/(max_val - min_val)\n",
        "    return norm_labels\n",
        "\n",
        "def test_accuracy(model, data_loader, loss_function):\n",
        "    start_time = time.time()\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "      for data, label in data_loader:\n",
        "          data, label = data.to(DEVICE), label.to(DEVICE)\n",
        "          logits, predictions = model(data)\n",
        "          int_labels = label.argmax(dim=1)\n",
        "          loss = loss_function(logits, int_labels)\n",
        "\n",
        "          epoch_loss += loss.item() * data.size(0)\n",
        "\n",
        "          pred_classes = torch.argmax(logits, dim=1)\n",
        "          true_classes = int_labels\n",
        "          correct += (pred_classes == true_classes).sum().item()\n",
        "\n",
        "          total += label.size(0)\n",
        "\n",
        "    avg_loss = epoch_loss / total\n",
        "    accuracy = correct / total * 100\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    return (100-accuracy), avg_loss"
      ],
      "metadata": {
        "id": "oCEATx5WhADe"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Training*"
      ],
      "metadata": {
        "id": "95LZ6uu14Byo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m_name = \"cnn_test_4\"\n",
        "\n",
        "cnn = ConvNetwork(len(LANGUAGES)).to(DEVICE)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "LEARNING_RATE=0.001\n",
        "optimizer = torch.optim.Adam(cnn.parameters(), lr=LEARNING_RATE, weight_decay=1e-6)\n",
        "\n",
        "train(cnn, m_name, train_loader, DEVICE, optimizer, loss_function, NUM_EPOCHS, val_loader)\n",
        "\n",
        "plot_training_curve(m_name)"
      ],
      "metadata": {
        "id": "1O7j-TPTyH4y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "outputId": "c48101dd-06b8-44ab-f553-c2395383441f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #1 | Loss: 1.5442555626276881 | Error: 70.4% | Loss: 1.5459291950265566 | Error: 71.06666666666666%\n",
            "Epoch #2 | Loss: 1.4732746078242394 | Error: 66.69999999999999% | Loss: 1.7105126106838384 | Error: 72.19999999999999%\n",
            "Epoch #3 | Loss: 1.421883938228258 | Error: 63.25% | Loss: 1.5759273800104856 | Error: 68.93333333333334%\n",
            "Epoch #4 | Loss: 1.300589627708562 | Error: 57.56666666666666% | Loss: 2.7333061303404076 | Error: 70.06666666666666%\n",
            "Epoch #5 | Loss: 1.2356955458795362 | Error: 53.96666666666667% | Loss: 2.1728073373097985 | Error: 74.4%\n",
            "Epoch #6 | Loss: 1.1754173383821627 | Error: 49.4% | Loss: 2.2242428495627027 | Error: 75.73333333333333%\n",
            "Epoch #7 | Loss: 1.0976774425925104 | Error: 44.983333333333334% | Loss: 2.488308773512045 | Error: 71.80000000000001%\n",
            "Epoch #8 | Loss: 1.0371459153387521 | Error: 41.266666666666666% | Loss: 3.420290289294176 | Error: 75.73333333333333%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-8174a19fa992>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplot_training_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-fedf49f1f9a1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, model_name, data_loader, device, optimizer, loss_function, num_epochs, val_loader)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Not important Before this (wave2vec)\n"
      ],
      "metadata": {
        "id": "U6vla6419N9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torchaudio"
      ],
      "metadata": {
        "id": "vkFBj6L9ccKa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d051ea3d-d631-4121-a15e-3851179713e6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchaudio) (2.6.0+cu124)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchaudio) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
        "\n",
        "model_name = \"facebook/wav2vec2-base\"\n",
        "\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(LANGUAGES),\n",
        "    output_hidden_states=True\n",
        ")\n",
        "\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px94ZYQYniRP",
        "outputId": "15c660a3-b609-43a5-fc00-0efe215bb204"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:315: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "\n",
        "def preprocess_audio(audio_path):\n",
        "    waveform, sample_rate = torchaudio.load(audio_path)\n",
        "    waveform = torchaudio.functional.resample(waveform, sample_rate, 16000)\n",
        "\n",
        "    inputs = feature_extractor(\n",
        "        waveform.squeeze().numpy(),\n",
        "        sampling_rate=16000,\n",
        "        max_length=16000 * 5,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    return inputs.input_values.squeeze(0)"
      ],
      "metadata": {
        "id": "TiAzzqfcnoVb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Wav2Vec2Classifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.wav2vec2 = model\n",
        "        self.freeze_feature_extractor()\n",
        "\n",
        "    def freeze_feature_extractor(self):\n",
        "        # freezes pretrained layers so it doesn't overfit\n",
        "        # b/c our dataset is so small.\n",
        "        for param in self.wav2vec2.wav2vec2.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        # raw waveform of shape(it's just [batch, 80000])\n",
        "        outputs = self.wav2vec2(x)\n",
        "        return outputs.logits"
      ],
      "metadata": {
        "id": "tIfZPDZcnvZR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def load_data(languages, train_batch, val_batch, randomOn=False):\n",
        "  train_data = []\n",
        "  val_data = []\n",
        "\n",
        "  train_audio = []\n",
        "  train_labels = []\n",
        "  train_sr = []\n",
        "  val_audio = []\n",
        "  val_labels = []\n",
        "  val_sr = []\n",
        "\n",
        "  one_hot = F.one_hot(torch.tensor([0, 1, 2, 3, 4]), num_classes=len(languages))\n",
        "\n",
        "  for i in range(len(one_hot)):\n",
        "    one_hot[i] = one_hot[i].to(dtype=torch.float)\n",
        "\n",
        "  for i in range(len(languages)):\n",
        "    # Load common voice 17 dataset training set with streaming, and enabling custom code (necessary to load dataset correctly)\n",
        "    if randomOn:\n",
        "      train_set = load_dataset(\"mozilla-foundation/common_voice_17_0\", languages[i], split=\"train\", streaming=True, trust_remote_code=True)\n",
        "      train_data.append(train_set.shuffle(buffer_size=train_batch, seed=random.randrange(1,100)))\n",
        "      val_set = load_dataset(\"mozilla-foundation/common_voice_17_0\", languages[i], split=\"validation\", streaming=True, trust_remote_code=True)\n",
        "      val_data.append(val_set.shuffle(buffer_size=val_batch, seed=random.randrange(1,100)))\n",
        "    else:\n",
        "      train_data.append(load_dataset(\"mozilla-foundation/common_voice_17_0\", languages[i], split=\"train\", streaming=True, cache_dir='/content/my_cache', trust_remote_code=True))\n",
        "      val_data.append(load_dataset(\"mozilla-foundation/common_voice_17_0\", languages[i], split=\"validation\", streaming=True, cache_dir='/content/my_cache', trust_remote_code=True))\n",
        "\n",
        "    it = iter(train_data[i])\n",
        "    it2 = iter(val_data[i])\n",
        "\n",
        "    for j in range(train_batch):\n",
        "      train_item = next(it)\n",
        "\n",
        "      if train_item:\n",
        "        train_audio.append(train_item['audio']['array'])\n",
        "        train_sr.append(train_item['audio']['sampling_rate'])\n",
        "        train_labels.append(i)\n",
        "\n",
        "    for j in range(val_batch):\n",
        "      val_item = next(it2)\n",
        "\n",
        "      if val_item:\n",
        "        val_audio.append(val_item['audio']['array'])\n",
        "        val_sr.append(val_item['audio']['sampling_rate'])\n",
        "        val_labels.append(i)\n",
        "\n",
        "\n",
        "    print(f\"Loaded {languages[i]}\")\n",
        "\n",
        "  return train_audio, train_labels, train_sr, val_audio, val_labels, val_sr"
      ],
      "metadata": {
        "id": "E3EUQ1Njnyuu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_batch(audio_data, sample_rates, batch_size):\n",
        "    audio_processed = []\n",
        "    TARGET_SR = 16000  # Wav2Vec2 requires 16kHz so we are locked in this sr\n",
        "    DURATION = 5\n",
        "    MAX_LENGTH = TARGET_SR * DURATION\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        audio_resampled = librosa.resample(\n",
        "            audio_data[i],\n",
        "            orig_sr=sample_rates[i],\n",
        "            target_sr=TARGET_SR\n",
        "        )\n",
        "\n",
        "        audio_trimmed, _ = librosa.effects.trim(audio_resampled, top_db=30)\n",
        "\n",
        "        if len(audio_trimmed) > MAX_LENGTH:\n",
        "            audio_trimmed = audio_trimmed[:MAX_LENGTH]\n",
        "        else:\n",
        "            padding = MAX_LENGTH - len(audio_trimmed)\n",
        "            audio_trimmed = np.pad(audio_trimmed, (0, padding), mode='constant')\n",
        "\n",
        "        audio_processed.append(audio_trimmed.astype(np.float32))\n",
        "\n",
        "    return audio_processed"
      ],
      "metadata": {
        "id": "FQMif4H2skBI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "class Wav2Vec2Dataset(Dataset):\n",
        "    def __init__(self, audio_list, label_list):\n",
        "        self.audio = audio_list  # List of raw waveforms (numpy arrays)\n",
        "        self.labels = label_list # List of integer labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.audio[idx], self.labels[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    audio_arrays = [item[0] for item in batch]\n",
        "    labels = torch.tensor([item[1] for item in batch], dtype=torch.long)\n",
        "\n",
        "    inputs = feature_extractor(\n",
        "        audio_arrays,\n",
        "        sampling_rate=16000,\n",
        "        padding=\"max_length\",\n",
        "        max_length=80000,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "    return inputs.input_values, inputs.attention_mask, labels"
      ],
      "metadata": {
        "id": "xWZm3j2bsm8f"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2ForSequenceClassification\n",
        "\n",
        "def train_wav2vec2():\n",
        "    model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "        \"facebook/wav2vec2-base-960h\",\n",
        "        num_labels=len(LANGUAGES)\n",
        "    )\n",
        "\n",
        "    for param in model.wav2vec2.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    train_dataset = Wav2Vec2Dataset(train_processed, train_labels)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, collate_fn=collate_fn, shuffle=True)\n",
        "\n",
        "    val_dataset = Wav2Vec2Dataset(val_processed, val_labels)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=collate_fn)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        correct = 0\n",
        "        total_acc = 0\n",
        "\n",
        "        for input_values, attention_mask, labels in train_loader:\n",
        "            input_values = input_values.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(input_values, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total_acc += labels.size(0)\n",
        "\n",
        "            loss = outputs.loss\n",
        "\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        if epoch == 2:\n",
        "            for param in model.wav2vec2.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "        val_acc, val_loss = evaluate_wav2vec2(model, val_loader, device, loss_fn)\n",
        "        train_acc = correct / total_acc * 100\n",
        "        print(f\"Epoch {epoch+1} | Train Loss: {total_loss/len(train_loader):.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "def evaluate_wav2vec2(model, loader, device, criterion):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_values, attention_mask, labels in loader:\n",
        "            input_values = input_values.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(input_values, attention_mask=attention_mask)\n",
        "\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return correct / total * 100, total_loss / total"
      ],
      "metadata": {
        "id": "aP8jM5fWsrsL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_audio, train_labels, train_sr, val_audio, val_labels, val_sr = load_data(\n",
        "    languages=LANGUAGES,\n",
        "    train_batch=1200,\n",
        "    val_batch=300,\n",
        "    randomOn=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqXypaf_ssQN",
        "outputId": "338dd41c-0352-4ce2-e2ca-ad9568fe319b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading metadata...: 888827it [00:24, 31805.04it/s]'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c8b3d132-2fe1-454c-9f77-0cdf669bbd8c)')' thrown while requesting GET https://huggingface.co/datasets/mozilla-foundation/common_voice_17_0/resolve/main/transcript/en/train.tsv\n",
            "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c8b3d132-2fe1-454c-9f77-0cdf669bbd8c)')' thrown while requesting GET https://huggingface.co/datasets/mozilla-foundation/common_voice_17_0/resolve/main/transcript/en/train.tsv\n",
            "Retrying in 1s [Retry 1/5].\n",
            "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
            "Reading metadata...: 1101170it [00:40, 26924.82it/s]\n",
            "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: cc7fb0c7-6a3e-4b88-8018-a12f344d46b8)')' thrown while requesting GET https://huggingface.co/datasets/mozilla-foundation/common_voice_17_0/resolve/main/audio/en/train/en_train_17.tar\n",
            "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: cc7fb0c7-6a3e-4b88-8018-a12f344d46b8)')' thrown while requesting GET https://huggingface.co/datasets/mozilla-foundation/common_voice_17_0/resolve/main/audio/en/train/en_train_17.tar\n",
            "Retrying in 1s [Retry 1/5].\n",
            "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
            "Reading metadata...: 16393it [00:02, 5837.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded en\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading metadata...: 589100it [00:17, 34122.58it/s]\n",
            "Reading metadata...: 16183it [00:01, 9456.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded de\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading metadata...: 34898it [00:01, 27860.91it/s]\n",
            "Reading metadata...: 11252it [00:01, 7751.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded nl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading metadata...: 7744it [00:00, 18880.48it/s]\n",
            "Reading metadata...: 5210it [00:01, 4646.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded sv-SE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading metadata...: 3484it [00:00, 11916.73it/s]\n",
            "Reading metadata...: 2105it [00:00, 7679.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded da\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_processed = process_batch(train_audio, train_sr, len(train_audio))\n",
        "val_processed = process_batch(val_audio, val_sr, len(val_audio))"
      ],
      "metadata": {
        "id": "u2qEpOwOuAH5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_wav2vec2()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "nL88fToMumy3",
        "outputId": "eed40f53-190f-4afe-c6e5-a23f4b473438"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train Loss: 1.4799 | Train Acc: 39.13% | Val Loss: 1.4367 | Val Acc: 40.00%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-13d19ee6abbb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_wav2vec2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-34527460d8ee>\u001b[0m in \u001b[0;36mtrain_wav2vec2\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m   2375\u001b[0m             \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2376\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2377\u001b[0;31m             \u001b[0mpadding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_feature_vector_attention_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2378\u001b[0m             \u001b[0mexpand_padding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadding_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2379\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mexpand_padding_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36m_get_feature_vector_attention_mask\u001b[0;34m(self, feature_vector_length, attention_mask, add_adapter)\u001b[0m\n\u001b[1;32m   1412\u001b[0m         \u001b[0;31m# these two operations makes sure that all values before the output lengths idxs are attended to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m         \u001b[0mattention_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lengths\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}